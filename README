Pipefish sends the result of your SQL statement to a tab-delimited file in hdfs. 

It doesn't create any intermediate or temporary files. 
It just reads the rows from the MySQL server and writes them to a file in HDFS.
At the end of the row-set, pipefish flushes and closes the hdfs file and
closes the MySQL connection.

Supply --overwrite to truncate the file instead of appending to it.

$ pf --help
args:
 --defaults_file='/path/to/my.cnf'
 --user='user'
 --host='host'
 --db='db name'
 --password='password'
 --hdfs_path='/path/to/file'
 --sql='sql statement'
 [--overwrite]
user, host, db, and password settings override those specified in defaults_file.


B U I L D   I N S T R U C T I O N S
===================================


On Ubuntu
=========
Set JAVA_HOME.

Install MySQL development libs.

Install Cloudera CDH4 packages.

Get the libhdfs header.

  The header file hdfs.h does not come with the CDH4 packages, so
  you will need to fetch it from somewhere. Here is one place:

  curl -o hdfs.h https://raw.github.com/apache/hadoop-common/trunk/hadoop-hdfs-project/hadoop-hdfs/src/main/native/libhdfs/hdfs.h

Build and install it.

  $ make
  $ sudo make install

  `make install' puts it in /usr/local/bin.


On a Mac
========
Install MySQL development libs.

Build the HDFS libs from a Cloudera tarball distribution

  I couldn't get this to work with CDH4.

  Get your gcc toolchain installed along with cmake, ant, and maven.
  Download and upack the CDH5 tarball.
  I used hadoop-2.2.0-cdh5.0.0-beta-1.tar.gz

  Build just the hdfs lib that we need:

  $ cd hadoop-2.2.0-cdh5.0.0-beta-1/src/hadoop-hdfs-project/hadoop-hdfs
  $ sudo mvn package -Pdist,native -DskipTests

  Built OK? Put the headers and libs in a place that GCC can find them:

  $ sudo cp `find target/native -name libhdfs\.*` /usr/local/lib
  $ sudo cp `find target -name hdfs.h` /usr/local/include

Build and install it.

  $ make
  $ sudo make install

  `make install' puts it in /usr/local/bin.
